{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission notebook for  exercise 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student name: Karan Sanjay Dhage\n",
    "#### student Id: 419197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data updated gathering and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/Source codes/Data_preparation/Data_preparation.py\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def JH_Data_repo():\n",
    "    \"\"\" Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predefined csv structure\n",
    "    \"\"\"\n",
    "    git_pull = subprocess.Popen(\"user/bin/JH_Data_gitPull\",\n",
    "                                cwd=os.path.dirname('data/raw/COVID-19/'),\n",
    "                                shell=True,\n",
    "                                stdout=subprocess.PIPE,\n",
    "                                stderr=subprocess.PIPE) # Change directory path as per your need\n",
    "    (out, error) = git_pull.communicate()\n",
    "\n",
    "    print(\"Error : \" + str(error))\n",
    "    print(\"out : \" + str(out))\n",
    "\n",
    "\n",
    "def Example_for_Germany():\n",
    "    \"\"\" Get current data from germany, attention API endpoint not too stable\n",
    "        Result data frame is stored as pd.DataFrame and later saved in CSV file in local drive\n",
    "\n",
    "    \"\"\"\n",
    "    data = requests.get(\n",
    "        'https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/RKI_Landkreisdaten/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json')\n",
    "\n",
    "    json_object = json.loads(data.content)  # load all data\n",
    "    German_data_list = []\n",
    "    for pos, each_dict in enumerate(json_object['features'][:]):\n",
    "        German_data_list.append(each_dict['attributes'])\n",
    "\n",
    "    df_Germany = pd.DataFrame(German_data_list)  # prepare a Dataframe containing final data\n",
    "    df_Germany.to_csv(r'C:\\Users\\HP\\Desktop\\pythonProject\\data/GER_state_data.csv', sep=';')  # save data as CSV file in needed folder\n",
    "    print(' Regions rows: ' + str(df_Germany.shape[0]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    JH_Data_repo()\n",
    "    Example_for_Germany()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/Source codes/JH_Data_Extraction/JH_Data_Extraction.py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def JH_data_Extraction():\n",
    "    \"\"\" Creation of relative data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    path = r'C:\\Users\\HP\\Desktop\\pythonProject/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series' \\\n",
    "           r'/time_series_covid19_confirmed_global.csv '\n",
    "    df_raw = pd.read_csv(path, delimiter=\",\")\n",
    "    df_db = df_raw.rename(columns={'Country/Region': 'country',\n",
    "                                   'Province/State': 'state'})\n",
    "\n",
    "    df_db['state'] = df_db['state'].fillna('no')\n",
    "\n",
    "    df_db = df_db.drop(['Lat', 'Long'], axis=1)\n",
    "\n",
    "    df_rel = df_db.set_index(['state', 'country']) \\\n",
    "        .T \\\n",
    "        .stack(level=[0, 1]) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'level_0': 'date',\n",
    "                         0: 'confirmed'},\n",
    "                )\n",
    "\n",
    "    df_rel['date'] = df_rel.date.astype('datetime64[ns]')\n",
    "\n",
    "    df_rel.to_csv(r'C:\\Users\\HP\\Desktop\\pythonProject\\data/COVID_relational_confirmed.csv', sep=';', index=False) # Change directory path as per your need\n",
    "    print(' Stored rows: ' + str(df_rel.shape[0]))\n",
    "    print(' Latest date is: ' + str(max(df_rel.date)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    JH_data_Extraction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/Source codes/Filtration of data/Data_Filtration.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "from scipy import signal\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "\n",
    "def doubling_T_via_reg(in_array):\n",
    "    \"\"\" Using a linear regression to calculate the doubling rate\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1, 2).reshape(-1, 1)  # shaping x, y arrays\n",
    "\n",
    "    assert len(in_array) == 3\n",
    "    reg.fit(X, y)  # regression\n",
    "    intercept = reg.intercept_\n",
    "    slope = reg.coef_\n",
    "\n",
    "    return intercept / slope\n",
    "\n",
    "\n",
    "def savgol_fil(input_data, column='confirmed', window=5):\n",
    "    \"\"\" Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "        parameters:\n",
    "        ----------\n",
    "        input_data : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the input_data has to be preserved in result\n",
    "    \"\"\"\n",
    "\n",
    "    degree = 1\n",
    "    df_result = input_data\n",
    "\n",
    "    filter_in = input_data[column].fillna(0)  # attention with the neutral element here\n",
    "\n",
    "    result = signal.savgol_filter(np.array(filter_in),\n",
    "                                  window,  # window size used for filtering\n",
    "                                  1)\n",
    "    df_result[str(column + '_filtered')] = result\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def rolling_reg(input_data, col='confirmed'):\n",
    "    \"\"\" Rolling Regression to approximate the doubling time'\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_data: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    \"\"\"\n",
    "    days_back = 3\n",
    "    result = input_data[col].rolling(\n",
    "        window=days_back,\n",
    "        min_periods=days_back).apply(doubling_T_via_reg, raw=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def filtered_data(input_data, filter_on='confirmed'):\n",
    "    \"\"\"  Calculate savgol filter and return merged data frame\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_data: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    \"\"\"\n",
    "\n",
    "    must_contain = set(['state', 'country', filter_on])\n",
    "    assert must_contain.issubset(set(input_data.columns)), ' Error in filtered_data not all columns in data frame'\n",
    "\n",
    "    output = input_data.copy()  # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result = output[['state', 'country', filter_on]].groupby(['state', 'country']).apply(\n",
    "        savgol_fil)  # .reset_index()\n",
    "\n",
    "    output = pd.merge(output, pd_filtered_result[[str(filter_on + '_filtered')]], left_index=True,\n",
    "                      right_index=True, how='left')\n",
    "    return output.copy()\n",
    "\n",
    "\n",
    "def doubling_rate(input_data, filter_on='confirmed'):\n",
    "    \"\"\" Calculate approximated doubling rate and return merged data frame\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_data: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    \"\"\"\n",
    "\n",
    "    must_contain = set(['state', 'country', filter_on])\n",
    "    assert must_contain.issubset(set(input_data.columns)), ' Error in filtered_data not all columns in data frame'\n",
    "\n",
    "    pd_DR_result = input_data.groupby(['state', 'country']).apply(rolling_reg, filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result = pd_DR_result.rename(columns={filter_on: filter_on + '_DR',\n",
    "                                                'level_2': 'index'})\n",
    "\n",
    "    # Merging on the index of our big table and on the index column after groupby\n",
    "    output = pd.merge(input_data, pd_DR_result[['index', str(filter_on + '_DR')]], left_index=True, right_on=['index'],\n",
    "                      how='left')\n",
    "    output = output.drop(columns=['index'])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # select directory path as per your folder structure\n",
    "    JH_data = pd.read_csv(r'/data/COVID_relational_confirmed.csv', sep=';', parse_dates=[0])\n",
    "    JH_data = JH_data.sort_values('date', ascending=True).copy()\n",
    "    result_larg = filtered_data(JH_data)\n",
    "    result_larg = doubling_rate(result_larg)\n",
    "    result_larg = doubling_rate(result_larg, 'confirmed_filtered')\n",
    "\n",
    "    mask_data = result_larg['confirmed'] > 100\n",
    "    result_larg['confirmed_filtered_DR'] = result_larg['confirmed_filtered_DR'].where(mask_data, other=np.NaN)\n",
    "    result_larg.to_csv(r'C:\\Users\\HP\\Desktop\\pythonProject\\data/processed/COVID_final_set.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final dashboard for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/Source codes/Visualisation of data/Dashboard.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "dash.__version__\n",
    "\n",
    "print(os.getcwd())\n",
    "df_input_large = pd.read_csv(r'C:\\Users\\HP\\Desktop\\pythonProject\\data\\processed\\COVID_final_set.csv', sep=';')\n",
    "\n",
    "fig = go.Figure()\n",
    "app = dash.Dash()\n",
    "app.layout = html.Div([\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    #  Applied Data Science on COVID-19 dataset\n",
    "    This dashboard is being implemented to demonstrate the understanding and knowledge gained during the data science \n",
    "    course. This will show different aspects of data science such as automated data scrapping, data filtration and \n",
    "    machine learning concepts.\n",
    "\n",
    "    '''),\n",
    "    dcc.Markdown('''\n",
    "    ## Selection of various countries for visualization purpose\n",
    "    '''),\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='countries',\n",
    "        options=[{'label': each, 'value': each} for each in df_input_large['country'].unique()],\n",
    "        value=['India', 'Germany', 'Poland'],  # which are pre-selected\n",
    "        multi=True\n",
    "    ),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "        ## Selection for Timeline of confirmed COVID-19 cases or the approximated doubling time\n",
    "        '''),\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='doubling_time',\n",
    "        options=[\n",
    "            {'label': 'Timeline Confirmed ', 'value': 'confirmed'},\n",
    "            {'label': 'Timeline Confirmed Filtered', 'value': 'confirmed_filtered'},\n",
    "            {'label': 'Timeline Doubling Rate', 'value': 'confirmed_DR'},\n",
    "            {'label': 'Timeline Doubling Rate Filtered', 'value': 'confirmed_filtered_DR'},\n",
    "        ],\n",
    "        value='confirmed',\n",
    "        multi=False\n",
    "    ),\n",
    "\n",
    "    dcc.Graph(figure=fig, id='main_window_slope'),\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('countries', 'value'),\n",
    "     Input('doubling_time', 'value')])\n",
    "def update_figure(country_list, show_doubling):\n",
    "    if 'doubling_rate' in show_doubling:\n",
    "        my_yaxis = {'type': \"log\",\n",
    "                    'title': 'Approximated doubling rate over 3 days (larger numbers are better #stayathome)'\n",
    "                    }\n",
    "    else:\n",
    "        my_yaxis = {'type': \"log\",\n",
    "                    'title': 'Logarithmic scaled Confirmed infected people (source johns hopkins csse)'\n",
    "                    }\n",
    "\n",
    "    traces = []\n",
    "    for each in country_list:\n",
    "\n",
    "        df_plot = df_input_large[df_input_large['country'] == each]\n",
    "\n",
    "        if show_doubling == 'doubling_rate_filtered':\n",
    "            df_plot = df_plot[\n",
    "                ['state', 'country', 'confirmed', 'confirmed_filtered', 'confirmed_DR', 'confirmed_filtered_DR',\n",
    "                 'date']].groupby(['country', 'date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot = df_plot[\n",
    "                ['state', 'country', 'confirmed', 'confirmed_filtered', 'confirmed_DR', 'confirmed_filtered_DR',\n",
    "                 'date']].groupby(['country', 'date']).agg(np.sum).reset_index()\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                           y=df_plot[show_doubling],\n",
    "                           mode='markers+lines',\n",
    "                           opacity=0.9,\n",
    "                           name=each\n",
    "                           )\n",
    "                      )\n",
    "    return {\n",
    "        'data': traces,\n",
    "        'layout': dict(\n",
    "            width=1280,\n",
    "            height=720,\n",
    "            title= \"Graphical representation of log scaled confirmed cases to timeline\"\n",
    "            xaxis={'title': 'Timeline',\n",
    "                   'tickangle': -45,\n",
    "                   'nticks': 20,\n",
    "                   'tickfont': dict(size=14, color=\"#7f7f7f\"),\n",
    "                   },\n",
    "\n",
    "            yaxis=my_yaxis\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
